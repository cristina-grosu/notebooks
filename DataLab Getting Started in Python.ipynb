{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Getting Started in Python2\n",
    "\n",
    "The Bigstep DataLab is a open data exploration service that offers data science, analytics and technology experimentation, built on our SparkArray, DataLake and on our highly flexible and high performance bare-metal infrastructure.\n",
    "\n",
    "This tutorial assumes some programming experience.\n",
    "\n",
    "## Uploading Data\n",
    "\n",
    "A private datalake (HDFS service) is used to store the data that the SparkArray uses. To upload data to an HDFS cluster one would typically:\n",
    "1. download the hadoop binaries (2.7.x) from a mirror like [here](http://apache.claz.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz)  - rather large  (240mb)\n",
    "2. unarchive \n",
    "3. execute commands like \"-ls\".\n",
    "\n",
    "```\n",
    "/hadoop-2.7.3/bin/hdfs dfs -ls hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/\n",
    "16/09/26 17:18:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 3 items\n",
    "drwxrwxrwx   - hdfs supergroup          0 2016-09-22 13:12 hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/baseball\n",
    "drwxrwxrwt   - hdfs supergroup          0 2016-09-22 12:08 hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/tmp\n",
    "drwxr-xr-x   - hdfs supergroup          0 2016-09-22 12:08 hdfs://headnodes-8885.cluster-8885.us-private-datalake.7.bigstep.io/user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also execute the same commands on the master container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://www.exploredata.net/ftp/MLB2008.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -put MLB2008.csv /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Context\n",
    "\n",
    "For all Spark functions to be available, a Spark context has to be initialized in the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext.getOrCreate(conf=SparkConf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "An Resilient Distributed Dataset is an array that is spread across multiple servers. It allows the programmer to abstract away the complexity of transforming large volumes of distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip\n",
    "\n",
    "apt-get install -y unzip\n",
    "unzip baseballdatabank-master_2016-03-02.zip\n",
    "rm -rf baseballdatabank-master_2016-03-02.zip\n",
    "\n",
    "hdfs dfs -put baseballdatabank-master/core/AllstarFull.csv /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"/AllstarFull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linesWithRuth = textFile.filter(lambda line: \"ruth\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithRuth.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithRuth.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junky junky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "linesWithRuth.saveAsTextFile(\"/lines_with_ruth-\"+str(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and SparkSQL\n",
    "\n",
    "Using the SparkSQL module, SQL queries can be run directly on the files that are stored in Bigstep DataLake in various data formats, such as Parquet, after initializing the SparkSQL context. The type that aggregates data for SQL is the dataframe, which is built on top of RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize SparkSQL context\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"demoNotebookSparkSQL\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfvsgvbgbrberrb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new junk here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark 2.1.0 has a built-in CSV reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -chmod 740 /AllstarFull.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allstar=spark.read.csv(\"/AllstarFull.csv\",header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(allstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allstar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#register this table as a \"table\" within the sql context.\n",
    "allstar.createOrReplaceTempView(\"allstar\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "player = spark.sql(\"SELECT * FROM allstar WHERE playerID like '%ruth%' and yearID<1935\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParquetFiles\n",
    "Parquet files are typically much faster and take up less space than CSVs and the DataLake supports them as well. As Spark is a clustering system the parquet files are composed out of many fragments generated by the workres independently. The collection of files is operated as a single big table by SparkSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "path=\"/allstar-\"+str(time.time())+\".parquet\"\n",
    "player.write.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfParquet = spark.read.parquet(path)\n",
    "dfParquet.createOrReplaceTempView(\"player\")\n",
    "spark.sql(\"SELECT playerID,YearID FROM player\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External packages\n",
    "\n",
    "There are a lot of packages that are included in the standard offering and can be seen under the 'Conda' tab in Jupyter notebook. However, we know that different use cases require a different packages in order to be able to perform advanced analytics, machine learning operations or complex visualizations. \n",
    "\n",
    "As a result, Bigstep SparkArray offers a simple solution to add custom packages in a specific notebook either by downloading it from the included library of installing it following the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced visualization using Plotly\n",
    "\n",
    "Graphs help every data science team create and diseminate a powerful story built on the data they are analyzing. \n",
    "\n",
    "Using the default visualization libraries or external ones beautiful graphs can be created. As an example, using a standard Plotly example we can have a sneak peek into USA's flight paths from 2011:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly\n",
    "from plotly.offline import iplot\n",
    "\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "df_airports = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2011_february_us_airport_traffic.csv')\n",
    "df_airports.head()\n",
    "\n",
    "df_flight_paths = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv')\n",
    "df_flight_paths.head()\n",
    "\n",
    "airports = [ dict(\n",
    "        type = 'scattergeo',\n",
    "        locationmode = 'USA-states',\n",
    "        lon = df_airports['long'],\n",
    "        lat = df_airports['lat'],\n",
    "        hoverinfo = 'text',\n",
    "        text = df_airports['airport'],\n",
    "        mode = 'markers',\n",
    "        marker = dict( \n",
    "            size=2, \n",
    "            color='rgb(255, 0, 0)',\n",
    "            line = dict(\n",
    "                width=3,\n",
    "                color='rgba(68, 68, 68, 0)'\n",
    "            )\n",
    "        ))]\n",
    "        \n",
    "flight_paths = []\n",
    "for i in range( len( df_flight_paths ) ):\n",
    "    flight_paths.append(\n",
    "        dict(\n",
    "            type = 'scattergeo',\n",
    "            locationmode = 'USA-states',\n",
    "            lon = [ df_flight_paths['start_lon'][i], df_flight_paths['end_lon'][i] ],\n",
    "            lat = [ df_flight_paths['start_lat'][i], df_flight_paths['end_lat'][i] ],\n",
    "            mode = 'lines',\n",
    "            line = dict(\n",
    "                width = 1,\n",
    "                color = 'red',\n",
    "            ),\n",
    "            opacity = float(df_flight_paths['cnt'][i])/float(df_flight_paths['cnt'].max()),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "layout = dict(\n",
    "        title = 'Feb. 2011 American Airline flight paths<br>(Hover for airport names)',\n",
    "        showlegend = False, \n",
    "        height = 800,\n",
    "        geo = dict(\n",
    "            scope='north america',\n",
    "            projection=dict( type='azimuthal equal area' ),\n",
    "            showland = True,\n",
    "            landcolor = 'rgb(243, 243, 243)',\n",
    "            countrycolor = 'rgb(204, 204, 204)',\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "fig = dict( data=flight_paths + airports, layout=layout )\n",
    "\n",
    "plotly.offline.iplot(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
